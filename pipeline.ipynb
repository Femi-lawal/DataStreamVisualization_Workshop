{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ff7795",
   "metadata": {},
   "source": [
    "Task 0 — Setup & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebab82c",
   "metadata": {},
   "source": [
    "### Setup and Utility Functions\n",
    "This section imports required libraries, defines constants, and provides utility functions for time conversion and contiguous run detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e811dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import psycopg2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "plt.switch_backend(\"agg\")  # safe for headless environments\n",
    "\n",
    "AXES = [f\"axis_{i}\" for i in range(1, 9)]\n",
    "META_AXES = [f\"axis_{i}\" for i in range(9, 15)]\n",
    "TIME_COL = \"Time\"\n",
    "TRAIT_COL = \"Trait\"\n",
    "\n",
    "def contiguous_runs(mask: np.ndarray) -> List[Tuple[int,int]]:\n",
    "    runs, in_run, start = [], False, 0\n",
    "    for i, val in enumerate(mask):\n",
    "        if val and not in_run: in_run, start = True, i\n",
    "        elif not val and in_run: runs.append((start, i-1)); in_run = False\n",
    "    if in_run: runs.append((start, len(mask)-1))\n",
    "    return runs\n",
    "\n",
    "def to_seconds(tseries: pd.Series) -> pd.Series:\n",
    "    t = pd.to_datetime(tseries)\n",
    "    return (t - t.iloc[0]).dt.total_seconds()\n",
    "\n",
    "def median_dt_seconds(tseries: pd.Series) -> float:\n",
    "    t = pd.to_datetime(tseries)\n",
    "    dt = t.diff().dt.total_seconds().dropna()\n",
    "    return float(dt.median() if len(dt) else 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f2ef4",
   "metadata": {},
   "source": [
    "🟦 Task 1 — Training Data from DB (Neon/Postgres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284833f8",
   "metadata": {},
   "source": [
    "### Database Handler\n",
    "This section defines the database configuration and a handler class to fetch training data snapshots from the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "963b97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"host\": \"ep-twilight-poetry-ad4klocb-pooler.c-2.us-east-1.aws.neon.tech\",\n",
    "    \"database\": \"neondb\",\n",
    "    \"user\": \"neondb_owner\",\n",
    "    \"password\": \"npg_w8YtZHyuePS7\",\n",
    "    \"port\": \"5432\",\n",
    "    \"sslmode\": \"require\",\n",
    "}\n",
    "\n",
    "class DBHandler:\n",
    "    def __init__(self, cfg: Dict):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def connect(self):\n",
    "        return psycopg2.connect(**self.cfg)\n",
    "\n",
    "    def fetch_training_snapshot(self, table: str = \"pm_dashboard\", limit: int = 50000) -> pd.DataFrame:\n",
    "        with self.connect() as conn:\n",
    "            query = f\"\"\"\n",
    "                SELECT timestamp_recorded AS \"{TIME_COL}\", {\", \".join([f'\"{a}\"' for a in AXES])}\n",
    "                FROM {table}\n",
    "                WHERE timestamp_recorded IS NOT NULL\n",
    "                ORDER BY timestamp_recorded ASC\n",
    "                LIMIT {limit};\n",
    "            \"\"\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "        df = df.rename(columns={\"timestamp_recorded\": TIME_COL})            \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118842f",
   "metadata": {},
   "source": [
    "🟦 Task 2 — Regression Models (Univariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904102fc",
   "metadata": {},
   "source": [
    "### Regression Model Classes\n",
    "Defines data structures and classes for fitting univariate linear regression models to each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9738ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AxisCoeffs:\n",
    "    axis: str\n",
    "    slope: float\n",
    "    intercept: float\n",
    "    r2: float\n",
    "\n",
    "class AxisModel:\n",
    "    def __init__(self, axis: str):\n",
    "        self.axis = axis\n",
    "        self.model = LinearRegression()\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, t_sec: np.ndarray, y: np.ndarray):\n",
    "        X = t_sec.reshape(-1, 1)\n",
    "        self.model.fit(X, y)\n",
    "        self.slope = float(self.model.coef_[0])\n",
    "        self.intercept = float(self.model.intercept_)\n",
    "        return self\n",
    "\n",
    "    def predict(self, t_sec: np.ndarray) -> np.ndarray:\n",
    "        return self.model.predict(t_sec.reshape(-1, 1))\n",
    "\n",
    "    def residuals(self, t_sec: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        return y - self.predict(t_sec)\n",
    "\n",
    "    def fit_report(self, t_sec: np.ndarray, y: np.ndarray) -> AxisCoeffs:\n",
    "        yhat = self.predict(t_sec)\n",
    "        ss_res = np.sum((y - yhat) ** 2)\n",
    "        ss_tot = np.sum((y - y.mean()) ** 2) if len(y) else 0.0\n",
    "        r2 = 1 - ss_res/ss_tot if ss_tot > 0 else np.nan\n",
    "        return AxisCoeffs(self.axis, self.slope, self.intercept, r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb862fd2",
   "metadata": {},
   "source": [
    "🟦 Task 3 — Residual Analysis & Threshold Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac89d0e",
   "metadata": {},
   "source": [
    "### Residual Analysis\n",
    "Analyzes model residuals to determine alert and error thresholds for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6e9df7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AxisThresholds:\n",
    "    axis: str\n",
    "    z_alert: float\n",
    "    z_error: float\n",
    "    MinC: float\n",
    "    MaxC: float\n",
    "    T_seconds: float\n",
    "    dt_s: float\n",
    "    resid_mu: float\n",
    "    resid_sigma: float\n",
    "\n",
    "class ResidualAnalyzer:\n",
    "    def __init__(self, dt_s: float, z_alert=1.65, z_error=2.57):\n",
    "        self.dt_s = dt_s\n",
    "        self.z_alert = z_alert\n",
    "        self.z_error = z_error\n",
    "\n",
    "    def discover(self, axis_model: AxisModel, t_sec: np.ndarray, y: np.ndarray) -> AxisThresholds:\n",
    "        resid = axis_model.residuals(t_sec, y)\n",
    "        mu = float(np.mean(resid))\n",
    "        sigma = float(np.std(resid, ddof=1)) if len(resid) > 1 else 1.0\n",
    "\n",
    "        # store in model for later\n",
    "        axis_model.resid_mu = mu\n",
    "        axis_model.resid_sigma = sigma\n",
    "\n",
    "        # Convert Z thresholds to current thresholds\n",
    "        minC = self.z_alert * sigma\n",
    "        maxC = self.z_error * sigma\n",
    "\n",
    "        # Compute sustain time (95th percentile run length of positive exceedances)\n",
    "        mask = resid - mu > minC\n",
    "        runs = contiguous_runs(mask)\n",
    "        lengths = [j - i + 1 for i, j in runs] or [3]\n",
    "        pts_thresh = int(np.quantile(lengths, 0.95))\n",
    "        T_seconds = max(3, pts_thresh) * self.dt_s\n",
    "\n",
    "        return AxisThresholds(axis_model.axis,\n",
    "                              self.z_alert, self.z_error,\n",
    "                              float(minC), float(maxC),\n",
    "                              float(T_seconds), self.dt_s,\n",
    "                              mu, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20527186",
   "metadata": {},
   "source": [
    "🟦 Task 4 — Trainer (Train models on DB data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d80bbd",
   "metadata": {},
   "source": [
    "### Model Training and Visualization\n",
    "Contains classes for training models, normalizing data, and generating regression and residual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b494c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self, outdir: str):\n",
    "        self.outdir = outdir\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    def plot_regression(self, axis: str, t_sec: np.ndarray, y: np.ndarray, yhat: np.ndarray, label=\"train\"):\n",
    "        plt.figure()\n",
    "        plt.scatter(t_sec, y, s=3, label=\"Observed\")\n",
    "        plt.plot(t_sec, yhat, color=\"red\", label=\"Regression\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Current (A)\")\n",
    "        plt.title(f\"{axis} – Regression Fit ({label})\")\n",
    "        plt.tight_layout()\n",
    "        fname = os.path.join(self.outdir, f\"{axis}_regression_{label}.png\")\n",
    "        plt.savefig(fname, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_residuals(self, axis: str, t_sec: np.ndarray, resid: np.ndarray, th, label=\"train\"):\n",
    "        plt.figure()\n",
    "        plt.plot(t_sec, resid, linewidth=1, label=\"Residuals\")\n",
    "        if th is not None:\n",
    "            plt.axhline(th.resid_mu + th.MinC, color=\"orange\", linestyle=\"--\",\n",
    "                        label=f\"Alert (μ+{th.z_alert}σ)\")\n",
    "            plt.axhline(th.resid_mu + th.MaxC, color=\"red\", linestyle=\"--\",\n",
    "                        label=f\"Error (μ+{th.z_error}σ)\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Residual (A)\")\n",
    "        plt.title(f\"{axis} – Residuals ({label})\")\n",
    "        plt.tight_layout()\n",
    "        fname = os.path.join(self.outdir, f\"{axis}_residuals_{label}.png\")\n",
    "        plt.savefig(fname, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, db: DBHandler, outdir=\"pm_outputs_local\"):\n",
    "        self.db = db\n",
    "        self.outdir = outdir\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        self.models = {}\n",
    "        self.thresholds = {}\n",
    "        self.scaler_std = None\n",
    "        self.scaler_mm = None\n",
    "\n",
    "    def load_training(self, table=\"robot_readings\", limit=50000, normalize=True) -> pd.DataFrame:\n",
    "        df = self.db.fetch_training_snapshot(table=table, limit=limit)\n",
    "        df[\"t_sec\"] = to_seconds(df[TIME_COL])\n",
    "\n",
    "        if normalize:\n",
    "            # Fit scalers on training data (only once!)\n",
    "            self.fit_scalers(df)\n",
    "            df_scaled, _ = self.transform(df)   \n",
    "            df_scaled[\"t_sec\"] = df[\"t_sec\"]    # keep time column aligned\n",
    "            df_scaled[TIME_COL] = df[TIME_COL]  # keep raw timestamp for reference\n",
    "            return df_scaled\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    def fit(self, df_train: pd.DataFrame):\n",
    "        t = df_train[\"t_sec\"].to_numpy()\n",
    "        dt_s = median_dt_seconds(df_train[TIME_COL])\n",
    "        analyzer = ResidualAnalyzer(dt_s)\n",
    "        viz = Visualizer(self.outdir)\n",
    "        coeffs = []\n",
    "\n",
    "        for axis in AXES:\n",
    "            y = df_train[axis].to_numpy()\n",
    "            m = AxisModel(axis).fit(t, y)\n",
    "            th = analyzer.discover(m, t, y)\n",
    "\n",
    "            # Save regression + residual plots\n",
    "            yhat = m.predict(t)\n",
    "            viz.plot_regression(axis, t, y, yhat)\n",
    "            resid = m.residuals(t, y)\n",
    "            viz.plot_residuals(axis, t, resid, th)\n",
    "\n",
    "            self.models[axis] = m\n",
    "            self.thresholds[axis] = th\n",
    "            coeffs.append(m.fit_report(t, y).__dict__)\n",
    "\n",
    "        pd.DataFrame(coeffs).to_csv(os.path.join(self.outdir, \"model_coeffs.csv\"), index=False)\n",
    "        pd.DataFrame([vars(th) for th in self.thresholds.values()]).to_csv(\n",
    "            os.path.join(self.outdir, \"thresholds.csv\"), index=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def fit_scalers(self, df_train: pd.DataFrame):\n",
    "        self.scaler_mm = MinMaxScaler().fit(df_train[AXES].values)\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        # Apply MinMax scaling\n",
    "        mm = pd.DataFrame(self.scaler_mm.transform(df[AXES]), columns=AXES)\n",
    "\n",
    "        # 🔑 Preserve timestamp column if present\n",
    "        if TIME_COL in df.columns:\n",
    "            mm[TIME_COL] = df[TIME_COL].values\n",
    "\n",
    "        # 🔑 Optionally preserve trait or other metadata\n",
    "        if TRAIT_COL in df.columns:\n",
    "            mm[TRAIT_COL] = df[TRAIT_COL].values\n",
    "\n",
    "        return mm, None\n",
    "\n",
    "    def normalize_dataframe(self, df: pd.DataFrame, method=\"minmax\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normalize a dataframe using previously fitted scalers.\n",
    "        method = \"minmax\" or \"standard\"\n",
    "        \"\"\"\n",
    "        if method == \"minmax\":\n",
    "            norm_values = self.scaler_mm.transform(df[AXES])\n",
    "        elif method == \"standard\":\n",
    "            norm_values = self.scaler_std.transform(df[AXES])\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'minmax' or 'standard'\")\n",
    "        \n",
    "        df_norm = df.copy()\n",
    "        df_norm[AXES] = norm_values\n",
    "        return df_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ac1b2",
   "metadata": {},
   "source": [
    "🟦 Task 5 — Synthetic Test Data (Optional in the event there is no synthetic data CSV file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba13a0",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation\n",
    "Generates synthetic test data based on trained models and thresholds for testing anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "220d7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataGenerator:\n",
    "    def __init__(self, models, thresholds, outdir=\"pm_outputs_local\"):\n",
    "        self.models = models\n",
    "        self.thresholds = thresholds\n",
    "        self.outdir = outdir\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    def generate(self, t_start, n_rows, dt_s):\n",
    "        t_sec = np.arange(n_rows, dtype=float) * dt_s\n",
    "        times = pd.to_datetime(t_start) + pd.to_timedelta(t_sec, unit=\"s\")\n",
    "        data = {TRAIT_COL: [\"current\"]*n_rows, TIME_COL: times}\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "        for axis, m in self.models.items():\n",
    "            trend = m.predict(t_sec)\n",
    "            th = self.thresholds[axis]\n",
    "            resid_std = max(0.1, th.MinC/1.65) if th.MinC > 0 else 0.1\n",
    "            syn = trend + rng.normal(0, resid_std, size=n_rows)\n",
    "            run_pts = max(3, int(th.T_seconds / th.dt_s))\n",
    "            if n_rows > 6*run_pts:\n",
    "                syn[100:100+run_pts] += th.MinC*1.2\n",
    "                syn[300:300+run_pts] += th.MaxC*1.2\n",
    "            data[axis] = syn\n",
    "\n",
    "        for m in META_AXES: data[m] = np.nan\n",
    "        df = pd.DataFrame(data)\n",
    "        path = os.path.join(self.outdir, \"synthetic_test.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f7287",
   "metadata": {},
   "source": [
    "🟦 Task 6 — Local Streaming Simulation (CSV only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2160f",
   "metadata": {},
   "source": [
    "### Local Streaming Simulation\n",
    "Simulates streaming of data from a CSV file, yielding one row at a time with a delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2ba770b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalStreamer:\n",
    "    def __init__(self, csv_path, delay_s=0.1):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.idx = 0\n",
    "        self.delay = delay_s\n",
    "\n",
    "    def step(self):\n",
    "        if self.idx >= len(self.df):\n",
    "            return None\n",
    "        row = self.df.iloc[self.idx]\n",
    "        self.idx += 1\n",
    "        time.sleep(self.delay)\n",
    "        return row.to_dict()\n",
    "\n",
    "    def run(self, max_steps=None):\n",
    "        rows = []\n",
    "        while True:\n",
    "            if max_steps and self.idx >= max_steps:\n",
    "                break\n",
    "            row = self.step()\n",
    "            if row is None: break\n",
    "            rows.append(row)\n",
    "        return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2fae8",
   "metadata": {},
   "source": [
    "🟦 Task 7 — Anomaly Detection (local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da469d",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "Detects alert and error events in streamed data based on model residuals and learned thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "553b716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Event:\n",
    "    axis: str\n",
    "    type: str\n",
    "    start: float\n",
    "    end: float\n",
    "    duration: float\n",
    "    z_mean: float\n",
    "    p_value: float\n",
    "\n",
    "class AnomalyDetector:\n",
    "    def __init__(self, models, thresholds, outdir=\"pm_outputs_local\"):\n",
    "        self.models = models\n",
    "        self.thresholds = thresholds\n",
    "        self.events = []\n",
    "        self.outdir = outdir\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    def detect(self, df: pd.DataFrame):\n",
    "        t_sec = to_seconds(df[TIME_COL]).to_numpy()\n",
    "        for axis, m in self.models.items():\n",
    "            y = df[axis].to_numpy()\n",
    "            resid = y - m.predict(t_sec)\n",
    "            th = self.thresholds[axis]\n",
    "\n",
    "            # Z-scores relative to training μ,σ\n",
    "            z = (resid - th.resid_mu) / max(th.resid_sigma, 1e-9)\n",
    "            run_pts = max(3, int(th.T_seconds/th.dt_s))\n",
    "\n",
    "            # Alerts\n",
    "            for i,j in contiguous_runs(z >= th.z_alert):\n",
    "                if (j-i+1) >= run_pts:\n",
    "                    z_mean = float(np.mean(z[i:j+1]))\n",
    "                    p = 1.0 - 0.5*(1 + math.erf(abs(z_mean) / math.sqrt(2)))\n",
    "                    self.events.append(Event(axis,\"ALERT\",t_sec[i],t_sec[j],\n",
    "                                             (j-i+1)*th.dt_s,z_mean,p))\n",
    "            # Errors\n",
    "            for i,j in contiguous_runs(z >= th.z_error):\n",
    "                if (j-i+1) >= run_pts:\n",
    "                    z_mean = float(np.mean(z[i:j+1]))\n",
    "                    p = 1.0 - 0.5*(1 + math.erf(abs(z_mean) / math.sqrt(2)))\n",
    "                    self.events.append(Event(axis,\"ERROR\",t_sec[i],t_sec[j],\n",
    "                                             (j-i+1)*th.dt_s,z_mean,p))\n",
    "\n",
    "    def save(self):\n",
    "        if not self.events: return None\n",
    "        df = pd.DataFrame([e.__dict__ for e in self.events])\n",
    "        path = os.path.join(self.outdir,\"events.csv\")\n",
    "        df.to_csv(path,index=False)\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d71f6",
   "metadata": {},
   "source": [
    "🟦 Task 8 — End-to-End Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff2154",
   "metadata": {},
   "source": [
    "### End-to-End Pipeline Driver\n",
    "Runs the full pipeline: loads data, trains models, generates or loads test data, streams data, detects anomalies, and generates verification plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3aab3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_on_synthetic(models, thresholds, df_verif, outdir=\"pm_outputs_local\"):\n",
    "    viz = Visualizer(outdir)\n",
    "    t_sec = to_seconds(df_verif[TIME_COL]).to_numpy()\n",
    "\n",
    "    for axis, m in models.items():\n",
    "        y = df_verif[axis].to_numpy()\n",
    "        yhat = m.predict(t_sec)\n",
    "        resid = y - yhat\n",
    "\n",
    "        th = thresholds[axis]\n",
    "\n",
    "        # Save regression and residual plots for verification data\n",
    "        viz.plot_regression(axis, t_sec, y, yhat, label=\"verify\")\n",
    "        viz.plot_residuals(axis, t_sec, resid, th, label=\"verify\")\n",
    "\n",
    "\n",
    "def run_end_to_end_local(\n",
    "    training_table=\"robot_readings\",\n",
    "    training_limit=20000,\n",
    "    outdir=\"pm_outputs_local\",\n",
    "    synthetic_rows=1000,\n",
    "    stream_steps=500,\n",
    "    test_csv_file=None\n",
    "):\n",
    "    db = DBHandler(DB_CONFIG)\n",
    "    trainer = Trainer(db, outdir)\n",
    "    \n",
    "    # 1️⃣ Load & normalize training data\n",
    "    df_train = trainer.load_training(training_table, training_limit, normalize=True)\n",
    "    print(df_train.columns)\n",
    "    if df_train.empty:\n",
    "        raise RuntimeError(\"No training data in DB\")\n",
    "\n",
    "    # 2️⃣ Train models on normalized data\n",
    "    trainer.fit(df_train)\n",
    "\n",
    "    # 3️⃣ Prepare test data\n",
    "    dt_s = median_dt_seconds(df_train[TIME_COL])\n",
    "    if test_csv_file is not None:\n",
    "        test_csv_path = os.path.join('./data', test_csv_file)\n",
    "        df_test = pd.read_csv(test_csv_path)\n",
    "        df_test_scaled, _ = trainer.transform(df_test)  # 🔑 normalize test set\n",
    "        df_test_scaled[TIME_COL] = df_test[TIME_COL]    # keep timestamp\n",
    "    else:\n",
    "        gen = SyntheticDataGenerator(trainer.models, trainer.thresholds, outdir)\n",
    "        df_test = gen.generate(df_train[TIME_COL].iloc[-1], synthetic_rows, dt_s)\n",
    "        df_test_scaled, _ = trainer.transform(df_test)\n",
    "\n",
    "    # 4️⃣ Stream & detect anomalies\n",
    "    streamer = LocalStreamer(test_csv_path, delay_s=0.01)\n",
    "    df_streamed = streamer.run(max_steps=stream_steps)\n",
    "    df_streamed_scaled, _ = trainer.transform(df_streamed)\n",
    "\n",
    "    detector = AnomalyDetector(trainer.models, trainer.thresholds, outdir)\n",
    "    detector.detect(df_streamed_scaled)  # detection on normalized residuals\n",
    "    ev_path = detector.save()\n",
    "    print(\"✅ Done. Events saved to\", ev_path)\n",
    "\n",
    "    verify_on_synthetic(trainer.models, trainer.thresholds, df_streamed_scaled, outdir)\n",
    "    print(\"✅ Verification plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bc343",
   "metadata": {},
   "source": [
    "🟦 Task 9 — Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44101fc",
   "metadata": {},
   "source": [
    "### Run the End-to-End Pipeline\n",
    "This cell executes the full pipeline using the specified training table and test CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dab86e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\femil\\AppData\\Local\\Temp\\ipykernel_12772\\1181825981.py:26: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "c:\\Users\\femil\\Documents\\PersonalProjects\\CNSTG\\AIML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['axis_1', 'axis_2', 'axis_3', 'axis_4', 'axis_5', 'axis_6', 'axis_7',\n",
      "       'axis_8', 'Time', 't_sec'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\femil\\Documents\\PersonalProjects\\CNSTG\\AIML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\femil\\Documents\\PersonalProjects\\CNSTG\\AIML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Events saved to pm_outputs_local\\events.csv\n",
      "✅ Verification plots generated\n"
     ]
    }
   ],
   "source": [
    "run_end_to_end_local(\n",
    "    training_table=\"pm_dashboard\",\n",
    "    training_limit=36000,\n",
    "    outdir=\"pm_outputs_local\",\n",
    "    stream_steps=36000,\n",
    "    test_csv_file=\"synthetic_unnormalized_with_anomalies.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
